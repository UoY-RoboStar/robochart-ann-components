
dotprod(<>, <>) = 0
dotprod(s1, s2) = (head(s1) * head(s2)) + (dotprod(tail(s1), tail(s2)))

remove(s, 0) = s
remove(s, n) = remove(tail(s), (n-1))

first(s, 0) = <>
first(s, n) = <head(s)> ^ first(tail(s), n-1)
 
lastn(s, i) = remove(s, (#s - i))

last(s) = if (#s == 1) then head(s) else last(tail(s))

annoutput_layer(pl, l, 0) = <>
annoutput_layer(pl, l, n) =
	annoutput_layer(pl, l, (n-1)) ^
	<sign((dotprod(pl, extract_weights_node(l, n)))
		  +
		  extract_biases(l, n))>

annoutput_full(0, n, in) = first(in, n)
annoutput_full(l, n, in) = 
		annoutput_full(l-1, layerSize(l-1), in) ^
		annoutput_layer(lastn(annoutput_full(l-1, layerSize(l-1), in), 
						      layerSize(l-1)),
				  l,
				  n)
				  
annoutput(l, n, in) = last(annoutput_full(l, n, in))

--Channel to add a single value to the output.
channel add_input:Value

--channel clear_input 

channel get_input:{<a,b,c,d,e,f> | a <- Value, b <- Value, c <- Value, d <- Value, e <- Value, f <- Value}

Input = let 
	Input(in) = 
		(#in < insize) & add_input?x -> Input(in ^ <x>) []
		(#in == insize) & get_input.in -> Input(in) 
	within
		Input(<>)

SANN_InputNode(i) = layerRes.0.i?x -> add_input.x -> SKIP

--No longer hard coded, recursive reads all in one at a time. 
SANN_Node(l, n) = get_input?in -> layerRes.l.n.annoutput(l, n, in) -> SKIP

--Wrong result, reading sequence backwards, 
SANN_Layer(l) = ; i: <1..layerSize(l)> @ SANN_Node(l, i)

SANN_InputLayer = ; i : <1..insize> @ SANN_InputNode(i)

--Layers, define recursion here,  
SANN_Layers = (SANN_InputLayer) ; 
	   (; i : <1..layerNo> @ SANN_Layer(i))

assert SANN_Layers :[deadlock-free]
assert SANN_Layers :[divergence-free]
assert SANN_Layers :[deterministic]

--It still repeats, without calling that, 
SANN = ( (SANN_Layers [| {|add_input, get_input |} |] Input) 
			\ {| add_input, get_input |} )

assert SANN :[deadlock-free]
assert SANN :[divergence-free]
assert SANN :[deterministic]


FixOrder = (; i : <0..layerNo> @
				; j : <1..layerSize(i)> @
					layerRes.i.j?x -> SKIP) ; FixOrder
				

ANN_Fixed = (FixOrder
			[| {| layerRes |} |] 
			(HiddenLayers [| {| layerRes.(layerNo-1) |} |] OutputLayer))
				 

assert ANN_Fixed [FD= SANN  
assert SANN [FD= ANN_Fixed 



--Binary ANN constants (Binarised Segway with core_real) -- 
Value = {0,1}
insize = 6
outsize = 3
layerstructure = <3, 1, 3> 
layerNo = #layerstructure 
maxSize = 6
weights = < 
		< 
			<1,0,0,0,0,0>, 
			<0,1,0,0,0,0>, 
			<0,0,1,0,0,0> 
		>, 
		< 
			<0,1,0>
		>, 
		< 
			<0>,
			<0>,
			<1>
		> 
	>
biases = < <0,0,1>, <0>, <0,1,0> >

--Another network, three internal nodes. 
-- Value = {0,1}
-- insize = 3
-- outsize = 3
-- layerstructure = <3, 3, 3> 
-- layerNo = #layerstructure 
-- maxSize = 3
-- weights = < 
-- 		<
-- 			<1,0,0>,
-- 			<0,1,0>, 
-- 			<0,0,1>
-- 		>,
-- 		< 
-- 			<1,0,0>,
-- 			<0,1,0>, 
-- 			<0,0,1>
-- 		>, 
-- 		< 
-- 			<1,0,0>,
-- 			<0,1,0>, 
-- 			<0,0,1>
-- 		> 
-- 	>
-- 
-- biases = < <0,0,0>, <0,0,0>, <0,0,0> >

--Final network parameters, 2 hidden layers, 
-- Value = {0,1}
-- insize = 3
-- outsize = 3
-- layerstructure = <3, 3, 3, 3> 
-- layerNo = #layerstructure 
-- maxSize = 3
-- weights = < 
-- 			<
-- 				<1,0,0>,
-- 				<0,1,1>, 
-- 				<0,0,1>
-- 			>,
-- 			< 
-- 				<1,0,0>,
-- 				<1,1,0>, 
-- 				<0,0,1>
-- 			>, 
-- 			< 
-- 				<1,0,0>,
-- 				<0,1,0>, 
-- 				<0,1,1>
-- 			>, 
-- 			< 
-- 				<1,0,0>,
-- 				<0,1,0>, 
-- 				<0,0,1>
-- 			> 
-- 		>
-- 
-- biases = < <0,0,0>, <0,0,0>, <0,0,0>, <0,0,0> >



-- CHANNELS --
channel layerRes:{0..layerNo}.{1..maxSize}.Value
channel nodeOut:{1..layerNo}.{1..maxSize}.{1..maxSize}.Value
channel end


-- PROCESSES --

NodeIn(layer, node, index) = layerRes.(layer - 1).index?x -> nodeOut.layer.node.index.(x * extract_weights(layer, node, index)) -> SKIP

Node(layer, node, inputSize) = (||| i:{1..inputSize} @ NodeIn(layer, node, i)) [| {| nodeOut.layer.node |} |] Collator(layer, node, inputSize) \ {| nodeOut |} 

HiddenLayer(layer, size, inputSize) = [| {| layerRes.(layer-1) |} |] i: {1..size} @ Node(layer, i, inputSize) 

HiddenLayers = || i : {1..(layerNo-1)} @ [ {| layerRes.(i-1), layerRes.i |} ] HiddenLayer(i, layerSize(i), layerSize(i-1))  

Collator(layer, node, index) = let
	C(layer, node, 0, sum) = layerRes.layer.node.sign(sum + extract_biases(layer, node)) -> SKIP
	C(layer, node, index, sum) = nodeOut.layer.node.index?n -> 
										C(layer, node, (index-1), (sum+n))
	within 
		C(layer, node, index, 0)

OutputLayer = [| {| layerRes.(layerNo-1) |} |] i: {1..outsize} @ Node(layerNo, i, layerSize(layerNo-1)) \ {| nodeOut |}

ANN = ( (HiddenLayers [| {| layerRes.(layerNo-1) |} |] OutputLayer) \ ANNHiddenEvts ) [|{|end|}|>SKIP

--With input events hidden.
ANNInH = ANN \ {| layerRes.0 |}
assert ANNInH :[deterministic]
 
-- with output events hidden. Is deterministic. 
ANNOutH = ANN \ {| layerRes.3 |}
assert ANNOutH :[deterministic]

--With all layerRes events visible. 
ANN_VISIBLE = ( (HiddenLayers [| {| layerRes.(layerNo-1) |} |] OutputLayer) ) [|{|end|}|>SKIP

--This is the repeting ANN, ANN which repeats 
ANNR = ( ( (HiddenLayers [| {| layerRes.(layerNo-1) |} |] OutputLayer) \ ANNHiddenEvts ) [|{|end|}|>SKIP ) ; ANNR

--This is repeting, with all layerRes events visible, ANN
ANNR_VISIBLE = ( ( (HiddenLayers [| {| layerRes.(layerNo-1) |} |] OutputLayer) ) [|{|end|}|>SKIP ) ; ANNR_VISIBLE


--NOTES: Accepts inputs in any order, presents output in any order, interleaved, based on environment. 
--ASSERTIONS
--Passed all with this 6 inputs, and 3 outputs. 

--Deterministic assertions
assert ANN :[deterministic[FD]] 
assert ANNR :[deterministic[FD]] 
assert ANN_VISIBLE :[deterministic[FD]] 
assert ANNR_VISIBLE :[deterministic[FD]]

--Input Order Proofs. 
--Processes
--CHANNELS
channel intInput:{1..insize}.Value
InputO = TakeInputs(0,0,0,0,0,0,{1..insize})

--TakeInputs process. 
TakeInputs(x1, x2, x3, x4, x5, x6, {}) = ProvideInputs(<x1,x2,x3,x4,x5,x6>, {1..insize})
TakeInputs(x1, x2, x3, x4, x5, x6, S) = 
	member(1, S) & layerRes.0.1?x -> TakeInputs(x,x2,x3,x4,x5,x6, diff(S, {1})) []
	member(2, S) & layerRes.0.2?x -> TakeInputs(x2,x,x3,x4,x5,x6, diff(S, {2})) []
	member(3, S) & layerRes.0.3?x -> TakeInputs(x1,x2,x,x4,x5,x6, diff(S, {3})) []
	member(4, S) & layerRes.0.4?x -> TakeInputs(x1,x2,x3,x,x5,x6, diff(S, {4})) []
	member(5, S) & layerRes.0.5?x -> TakeInputs(x1,x2,x3,x4,x,x6, diff(S, {5})) []
	member(6, S) & layerRes.0.6?x -> TakeInputs(x1,x2,x3,x4,x5,x, diff(S, {6})) 
	
ProvideInputs(s, {}) = SKIP
ProvideInputs(s, S) = |~| i : S @ intInput.i.(extract_sequence(i, s)) -> ProvideInputs(s, diff(S,{i}))

ANNAnyOrder = 
	( ANN [[ layerRes.0.i.val <- intInput.i.val | i <- {1..insize}, val <- Value ]])
		[| {| intInput |} |] 
	( InputO )
	\ {| intInput |} 

assert ANN [FD= ANNAnyOrder
assert ANNAnyOrder [FD= ANN


--HELPER FUNCTION and other constants --

--Sign is the sign binary activation function.
sign(x) = if (x > 0) then 1 else 0

ANNHiddenEvts = diff(Events, {| layerRes.0, layerRes.3, end |})

-- Extraction Functions, because random access not implemented in CSPM, implemented as lists not a type of function --

extract_sequence(1, sequence) = head(sequence)
extract_sequence(index, sequence) = extract_sequence((index-1), tail(sequence))

layerSize(0) = insize
layerSize(layer) = extract_sequence(layer, layerstructure)

--Extract single weights value
extract_weights(layer, node, index) = extract_sequence(index, 
										(extract_sequence(node, 
											(extract_sequence(layer, weights)))))
--Extract weights of node
extract_weights_node(layer, node) = extract_sequence(node, 
										(extract_sequence(layer, weights)))
extract_biases(layer, node) = (extract_sequence(node, 
											(extract_sequence(layer, biases))))
