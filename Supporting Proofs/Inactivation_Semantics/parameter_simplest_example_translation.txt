WEights of the first 10 nodes in layer 1. 

Rearrange them into two symmetric layers of 5 nodes each: 

Layer 1: 

5.40062e-02,-2.61092e+00,
-1.12374e+00,2.63619e-02,


We want the conditions under which

Translate to INPUT CONDITIONS, PREDICATES JUST ON THE INPUT, NO CONNECTIONS AT ALL. 

Take two inputs, 

-0.5, 0.25, 0, 0.25, 0.5. 

Have conditions on these. 

Assume the weight already there. 

INACTIVATION, conditions that we can guarantee inactivation, that is what we want. 

What we are doing for the rest, conditions that we CAN INACTIVATE, purpose, prove inactivation of one of the output nodes. 

This is going to be quite, compare, (5) ^ (input number), for every node, but we will probably terminate before that. 

WE will most likely run out of inactivation conditions before we reach this, we start at the greatest, work our way downwards. 

Quite expensive, but its only ran once, do it in python? Proof of concept? 

Operation on lists? 

function on lists? 

Generate the PREDICATES, ON EACH INPUT, PREDICATES, EACH DISJUNCTIVE, IN DISJUNCTIVE NORMAL FORM? OR CNF? predicates that represent 
when we can guarantee inactivity. 

These will be on normalised values, for a real network, for the circus sematnics, so add the normalisation.

We need a first hidden layer, then the output layer? 

IN1: 
-0.5
Min value: -0.027

For negative weights, take the most positive: 
In2:
0.5,  -1.305
Inactive. 

0.227, 



Active at those, 

Layer 2: 

-3.41918e-02,1.49570e+00,
4.02002e-01,2.24022e-01,

inactive,active,
active,active,

Biases for layer 1: 
2.27630e-01,
-1.88762e-01,

active,
inactive,

Biases for layer 2: 

-5.88651e-01,
7.46065e-02,

inactive,
active,


MAX VALUES; 
Layer 1: N1: 1.5600931. N2: 1.691203,
Layer 2: N1: 1.9408813271, N2: 1.01306065

Maximum possible values. 

Minimum value, is now 0, range is 0..1.5600931, 

FOR RELU networks, and sigmoid, we can now IGNORE NEGATIVE WEIGHTS IN CALCULATING THE MAX. 

PREDICATES FOR LAYER 2: 

negative bias, 

All negative weights, of any combination, is in the semantics: 
- (node 1). 

Lowest positive weight, 1.49, * max of L1N2, which is 2.51, that is > bias.

So that is not in the semantics. 

Inactivation semantics is: 

- N1 always. No others. No different to normal sematnics, because inactive connections are always inactive, so no difference. 

Node 2; 

positive bias, but barely: 

negative weight: none. 

TAUTOLOGY. 

FOR RELU AND SIGMOID: 

If we have a positive bias AND ALL POSITIVE WEIGHTS, THAT NODE IS A TAUTOLOGY, ALWAYS ACTIVE. 

If we have negative bias and ALL NEGATIVE WEIGHTS, THAT NODE 

so start with LOWEST positive weight: 

Positive weights, have to be positive too, no way they can become negative, IF THEY ARE UNCERTAIN, 

IF THE RESULT UNCERTAIN, THE EDGES REDUCE THAT UNCERTAINTY. 

Edges only things that matter, in a network? 

Analysing the edge behaviour, this doesn't even matter. 

NO but a positive and 

Negative, 0, and positive, MAKES NEGATIVE, 0, doesn't always make positive. 

We may need a zero, as well, positive can be 0, or positive. 

Negative can just be negaitve. 

Even if its 0, doesn't matter, we can still 

positive could include 0? No. 

Uncertain, IF WE GET GIVEN UNCERTAIN, IT DOESN'T MEAN NEGATIVE, it can't mean negative, BECAUSE WE WOULD HAVE INACTIVE. 

UNCERTAIN MEANS 0 OR POSITIVE, so if we have alluncertain or positive, ON A POSITIVE NODE, THAT IS AN ACTIVATION CONDITION. 




