--This is the simplest possible network, it is a reduction even further on the Acas Xu first few nodes.
--inputs: 2, normalised with -0.5..0.5. 
--outputs: 2. 
--hidden layer with TWO NODES, then an output layer with two nodes. 
		
--SEMANTIC CONSTANTS--
insize = 2
outsize = 2
layerstructure = <2, 2> 
layerNo = #layerstructure 
maxSize = 2


--CHANNELS AND DATA TYPES: 

--Context channel: 

core_real = { -2..2}

datatype Phases = active | inactive | uncertain

channel NodePhase:{0..layerNo}.{1..maxSize}.Phases

--This is the intermediate channel, weight concatenation channel, 
channel EdgePhase:{1..layerNo}.{1..maxSize}.{1..maxSize}.Phases

channel end 

weights = 
	<
		< 
			<active,inactive>,
			<inactive,active>
		>,

		<
			<inactive,inactive>,
			<active,active>
		> 
	>

	


biases = 
	< 
		<active,inactive>,
		<inactive,active>
	>

--This HOLDS FOR ACTIVATION FUNCTIONS that have a minimum at 0, the uncertainty reduction. 
--INACTIVE for an edge means == 0, for relu. 
--If we have INACTIVE WEIGHT, ALWAYS AN INACTIVE EDGE. 
--If we have AN ACTIVE WEIGHT, and an ACTIVE input ==> ACTIVE, MUST BE (> 0).
--Uncertainty, if we are uncertain and have an ACTIVE weight, then UNCERTAIN, could be positive or 0, 
--Uncertainty, if we have an INACTIVE weight though, certain, MUST be inactive, that is, 0 or negative. 

--Uncertainty in a node means 0 or greater.
--Uncertainty in an EDGE means 0 or greater as well. 

--Inactivity means 0 OR LESS. 

--Activity MEANS STRICTLY GREATER THAN 0. 

EdgeLogic(input_phase, weight) = 
	if(weight == inactive) 
		then 
			inactive 
		else 
			if (weight == active) 
				then 
					if (input_phase == active)
						then
						active
					else if (input_phase == inactive)
						then
						uncertain
					else
						uncertain
				else 
					inactive
							
									
					
					

NodeEdge(layer, node, index) = 
	NodePhase.(layer - 1).index?phase ->
	
	EdgePhase.layer.node.index!(EdgeLogic(phase, extract_weights(layer, node, index))) -> 
	
	SKIP

--This needs to be different for every single node. We need to define quite a large function for this. 
--For every l and n, this is the core of the idea. 
--Needs to be a sequence, not a set. Also, we need a different behaviour for first hidden layer, because
--The input will be quantized, it won't be active or inactive. 

--Logic for positive biases, already have it, we don't need any additional logic. 

--Uncertainty, MEANS >= 0, here, 
--Active, means > 0
--INACTIVITY MEANS NEGATIVITY HERE. 
--Uncertainty, ALWAYS MEANS >= 0, because 

--Already have that, 
ActivationLogic(edge_results, bias) = 
	(not ( member(inactive, edge_results) ) and (bias == active)) 
	
--If we DON'T have any > 0, uncertainty, could mean >= 0, 
--Inactive bias, you can't have uncertain results, could have been the result of active weight, and uncertain, then could have activity.
--We cannot have uncertain values here, it could be positive. 
--Idea is we add to this as much as possible, with the algorithm and max. Remove uncertainty, if any uncertainty exists, we cannot guarantee inactivation. 
--But we can get inactivation, from uncertainty, easier, than we can get activation. 
--Going for this, add more conditions. 
--Conditions will be OR this, this is the base case, we will add more tolerances, that we can guarantee inactivation under some activity or uncertainty. 
InactivationLogic(edge_results, bias) = 
	(not ( member(active, edge_results) ) and not ( member(uncertain, edge_results)) and (bias == inactive))
	

EdgeCollator(layer, node, index) = let
	C(layer, node, 0, edge_results) = 
		(ActivationLogic(edge_results, extract_biases(layer, node)) & 
			NodePhase.layer.node!active -> SKIP)
		[] 
		(InactivationLogic(edge_results, extract_biases(layer, node)) &
			NodePhase.layer.node!inactive -> SKIP) 
			
		[] 
		(not(ActivationLogic(edge_results, extract_biases(layer, node))) and not(InactivationLogic(edge_results, extract_biases(layer, node))) & 
			( 
			(NodePhase.layer.node!uncertain -> SKIP) 
			)
		)
	
	C(layer, node, index, edge_results) = EdgePhase.layer.node.index?edge_phase -> 
										C(layer, node, (index-1), union(edge_results, {edge_phase}))
	within 
		C(layer, node, index, {})


Node(layer, node, inputSize) = 
	(||| i:{1..inputSize} @ NodeEdge(layer, node, i)) 
		[| {| EdgePhase.layer.node |} |]
	EdgeCollator(layer, node, inputSize) \ {| EdgePhase |} 


HiddenLayer(layer, size, inputSize) = 
	[| {| NodePhase.(layer-1) |} |] i: {1..size} @ Node(layer, i, inputSize) 

HiddenLayers = 
	|| i : {1..(layerNo-1)} @
		[ {| NodePhase.(i-1), NodePhase.i |} ] 
		HiddenLayer(i, layerSize(i), layerSize(i-1))  

OutputLayer = [| {| NodePhase.(layerNo-1) |} |] 
			i: {1..outsize} @ Node(layerNo, i, layerSize(layerNo-1))


ANNHiddenEvts = diff(Events, {| NodePhase.0, NodePhase.layerNo, end |})

InputLayer = 
	(||| i : {1..insize} @ ( (NodePhase.0.i.active -> SKIP) [] (NodePhase.0.i.inactive -> SKIP) ) )

--Going to flip the network, and add an interpreter, put the outputlayer first, that is the first hidden layer. 
--The output layer, may have a different structure, but that's fine, 
--No, because also, channel number. 
--Let's add a first hidden layer. Put it in the input layer? No, needs to correspond to the first hidden. 
--It is the first hidden layers behaviour, remove the input layer though. This first layer,
--Remove this inputlayer though, rename to first hidden. 
--Different predicate? it has a diferent channel as well, it does, outputs on NodePhase, but input is a different channel.
--It's edges are different, operate on a different channel, different type. 
--Add the output interpreter, to output on one channel, its decision, at the end as well. 
ANN = (( ( InputLayer [| {| NodePhase.0 |} |] HiddenLayers ) [| {| NodePhase.(layerNo-1) |} |] OutputLayer) \ ANNHiddenEvts) ; ANN 




--We need a VISIBLE EVENTS VERSION. 
ANN_Visible = (( ( InputLayer [| {| NodePhase.0 |} |] HiddenLayers ) [| {| NodePhase.(layerNo-1) |} |] OutputLayer)) ; ANN 

--Proving node 2.2 is ALWAYS ACTIVE. 
assert ANN_Visible \ {NodePhase.2.2.inactive } [T= ANN_Visible
assert ANN_Visible \ {NodePhase.2.2.uncertain } [T= ANN_Visible

--Proving node 2.1 is always inactive. 
assert ANN_Visible \ {NodePhase.2.1.active } [T= ANN_Visible
assert ANN_Visible \ {NodePhase.2.1.uncertain } [T= ANN_Visible


--In reality, this would be under a limited subset, under a condition, or we can find the input conditions.

--We can find the safe input conditions, which is all traces which lead to being active. The trace that fails. 

--Which is the refinement counterexample, where the specification is what you want, 

assert ANN_Visible :[divergence-free]
assert ANN :[divergence-free]
assert ANN :[deadlock-free]

--System property, interprets the 

--Put the normalisation, on, as well, build it properly? 

--Still don't know what to do about the input. 

--HELPER FUNCTIONS: 
-- Extraction Functions, because random access not implemented in CSPM, implemented as lists not a type of function --

extract_sequence(1, sequence) = head(sequence)
extract_sequence(index, sequence) = extract_sequence((index-1), tail(sequence))

layerSize(0) = insize
layerSize(layer) = extract_sequence(layer, layerstructure)

--Extract single weights value
extract_weights(layer, node, index) = extract_sequence(index, 
					(extract_sequence(node, 
						(extract_sequence(layer, weights)))))
--Extract weights of node
extract_weights_node(layer, node) = extract_sequence(node, 
							(extract_sequence(layer, weights)))
extract_biases(layer, node) = (extract_sequence(node, 
											(extract_sequence(layer, biases))))
											
										
