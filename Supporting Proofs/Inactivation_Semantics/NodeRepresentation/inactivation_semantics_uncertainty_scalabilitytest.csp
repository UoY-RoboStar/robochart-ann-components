--THis is a test, to see how big of a network we can construct

--And establish a single trace of, with the UNCERTAINTY version. 

--Weights, replace them all with positive, and biases with positive. 

--THis is best case, no sequences, 

--Started deadlock freedom check: 21:26

--Number of processes: 

--how many per node: 
--1 collator + previous layer/

--(number of nodes * (1 + previous-layer-size)


--layer 1: 250, layer 2: 2500, 
--2, 3, 4, 5, 6, 4, 5, 12,500
--250, 

--13,000 processes, not that much actually, how are we getting 15 million? 

--for every possible value, for the sequence, 

--Main calculation, then inputlayer, then the layers are just combinatinos. 

--SEMANTIC CONSTANTS--
insize = 5
outsize = 5
layerstructure = <5, 5, 5, 5, 5, 5, 5>
layerNo = #layerstructure 
maxSize = 5

   dfsag       35 2 zerox
   
   
--CHANNELS AND DATA TYPES: 

--Context channel: 

core_real = { -1..1}

--Decisions: 

datatype decisions = coc | wl | wr | sl | sr 

channel context_input:{1..insize}.core_real
--We expect a single output of all possible decisions, for every trace. 
channel context_output:Set({coc, wl, wr, sl, sr})


datatype Phases = active | inactive | zero | uncertain

channel NodePhase:{0..layerNo}.{1..maxSize}.Phases

--This is the intermediate channel, weight concatenation channel, 
channel EdgePhase:{1..layerNo}.{1..maxSize}.{1..maxSize}.Phases

channel end 

--4 inactive nodes, try to make them all active, 
--won't have a condition. 

--This HOLDS FOR ACTIVATION FUNCTIONS that have a minimum at 0, the uncertainty reduction. 
--Add something to this, if we are in the FIRST layer, don't do anything, return input_phase. 
EdgeLogic(input_phase, weight, first) = 
	if(first == true) then 
		if(input_phase == inactive)
			then 
				if(weight == inactive)
					then 
						active
					else
						inactive
			else
				if(weight == inactive)
					then
						inactive
					else
						active
	else 
		if(input_phase == inactive)
			then 
				zero
			else if (input_phase == active)
				then 
				(if(weight == inactive)
					then
						inactive
					else
						active)
			else
				(if(weight == inactive)
					then
						inactive
					else
						active)
							
									
					
					

--Behaves differently if its the first layer, because on the inputs, not on the weighted inputs. 
NodeEdge(layer, node, index) = 
	NodePhase.(layer - 1).index?phase ->
	( (layer == 1) & EdgePhase.layer.node.index!(EdgeLogic(phase, extract_weights(layer, node, index), true)) -> SKIP
	[]
	  (layer > 1) & EdgePhase.layer.node.index!(EdgeLogic(phase, extract_weights(layer, node, index), false)) -> SKIP
	) 
	

--This needs to be different for every single node. We need to define quite a large function for this. 
--For every l and n, this is the core of the idea. 
--The parameterised versions, has to be: 
--We have the layer and node, this is what it is based on: 
--It is different, explicit, for every dsifferent 
--Need the sequence extraction, extract_sequence(n, edge_results)

--These are not the conditions under which WE ARE UNSURE, activation logic: 

--FOr now, check why activationlogic(1,4) ever was being called: 


Activation(l,n,edge_results) = false

InActivation(l,n,edge_result) = false



--If activation logic is FALSE, then it is active.
--Activation Logic means we are UNSURE, TRUE IS UNSURE, FALSE IS ACTIVE.  
EdgeCollator(layer, node, index) = let
	C(layer, node, 0, edge_results) = 
	
		((extract_biases(layer,node) == active) & ( 
				((InActivation(layer, node, edge_results) == true) &(
					NodePhase.layer.node!uncertain -> SKIP ))
				[] 
				((InActivation(layer, node, edge_results) == false) &
					NodePhase.layer.node!active -> SKIP)
			)
		) 
		[]
		((extract_biases(layer,node) == inactive) & (
			((Activation(layer, node, edge_results) == true) & (
				NodePhase.layer.node!uncertain -> SKIP )) 
			[]
			((Activation(layer, node, edge_results) == false) & 
				NodePhase.layer.node!inactive -> SKIP)
			)
		)
			
	C(layer, node, index, edge_results) = EdgePhase.layer.node.index?edge_phase -> 
										C(layer, node, (index-1), <edge_phase>^edge_results)
	within 
		C(layer, node, index, <>)


Node(layer, node, inputSize) = 
	(||| i:{1..inputSize} @ NodeEdge(layer, node, i)) 
		[| {| EdgePhase.layer.node |} |]
	EdgeCollator(layer, node, inputSize) \ {| EdgePhase |} 
	

HiddenLayer(layer, size, inputSize) = 
	[| {| NodePhase.(layer-1) |} |] i: {1..size} @ Node(layer, i, inputSize) 

HiddenLayers = 
	|| i : {1..(layerNo-1)} @
		[ {| NodePhase.(i-1), NodePhase.i |} ] 
		HiddenLayer(i, layerSize(i), layerSize(i-1))  

OutputLayer = [| {| NodePhase.(layerNo-1) |} |] 
			i: {1..outsize} @ Node(layerNo, i, layerSize(layerNo-1))


ANNHiddenEvts = diff(Events, {| NodePhase.0, NodePhase.layerNo, context_output, end |})

--We can remove this, no, for analysis without another component, add this as context. 

--Doesn't make 0 avaliable, external choice, as it should be, waits on the context to decide one of the other, then 
--It passes them to the first layer. Triggers the first layer. 
--fixed to just inactive for now. 
InputLayer = 
	(||| i : {1..insize} @ ( NodePhase.0.i.active -> SKIP ))

--Fix all to active for now: 


--(NodePhase.0.i.active -> SKIP) [] 

--Directly from the paper, this is the acas xu. 
context_translation(1) = coc
context_translation(2) = wl
context_translation(3) = wr 
context_translation(4) = sl
context_translation(5) = sr 

emptylogic(false, _) = false 
emptylogic(true, x) = if (x == active or x == uncertain) then false else true 

--Operates on a constant channel. 
Output_Interpreter = let	
	C(0, network_results, empty) = 
		((empty) & context_output!{coc,wl,wr,sl,sr} -> SKIP )
		[]
		((not empty) & context_output!({ context_translation(j) | j <- {1..#network_results}, extract_sequence(j, network_results) == active or extract_sequence(j, network_results) == uncertain }) -> SKIP )
	
	C(index, network_results, empty) = NodePhase.layerNo.index?output -> C((index-1), <output>^network_results, emptylogic(empty, output))
										
	within 
		C(outsize, <>, true)

--Better if the output interpreter inside the main function, needs to be because of the recursion.
ANN = (( ( (InputLayer [| {| NodePhase.0 |} |] HiddenLayers)  
		[| {| NodePhase.(layerNo-1) |} |] 
	   OutputLayer) [| {| NodePhase.layerNo |} |] Output_Interpreter ) \ ANNHiddenEvts) ; ANN 

	  
Test = (OutputLayer) [| {| NodePhase.layerNo |} |] Output_Interpreter
-- ; ANN
--

ANN_Visible = ( (InputLayer [| {| NodePhase.0 |} |] HiddenLayers)  
		[| {| NodePhase.(layerNo-1) |} |] 
	   OutputLayer)  ; ANN_Visible

zero_norm(x) = if (x == active) then 1 else if (x == inactive) then -1 else 0

transparent diamond
transparent sbisim
ANNRenamed = sbisim(diamond(ANN[[NodePhase.0.i.x <- context_input.i.zero_norm(x)
	      | x <- Phases, i <- {1..insize} ]]))

assert ANNRenamed :[deadlock-free[FD]]
assert ANNRenamed :[divergence-free]
assert ANNRenamed :[deterministic[FD]]

--This HOLDS, 2.1 can never be active, as long as all inputs are active. 
assert ANN_Visible \ {NodePhase.2.1.active} [T= ANN_Visible

assert ANNRenamed \ {| context_output |} [T= ANNRenamed
assert ANN \ {| context_output |} [T= ANN
--To find out the result:




--HELPER FUNCTIONS: 
-- Extraction Functions, because random access not implemented in CSPM, implemented as lists not a type of function --

extract_sequence(1, sequence) = head(sequence)
extract_sequence(index, sequence) = extract_sequence((index-1), tail(sequence))

layerSize(0) = insize
layerSize(layer) = extract_sequence(layer, layerstructure)

--Extract single weights value
--extract_weights(layer, node, index) = extract_sequence(index, 
					--(extract_sequence(node, 
						--(extract_sequence(layer, weights)))))
--Extract weights of node
--extract_biases(layer, node) = (extract_sequence(node, 
--											(extract_sequence(layer, biases))))
		
extract_weights(layer, node, index) = active 
extract_biases(layer, node) = active
										
