--Inactivation semantics, where each layer is captured instead of each node. 
--We represent the edges using functions, instead of processes. 

--This is a test, running on the size and shape of the actual acas xu network.
--With all weights and biases active, and the default logic for inactivation, just a test.

--The ACTIVE VERSION PASSES IN SECONDS, good, but we do really need the real version now. 

--SEMANTIC CONSTANTS--
insize = 5
outsize = 5
layerstructure = <50, 50, 50, 50, 50, 50, 5>
layerNo = #layerstructure 
maxSize = 50


--CHANNELS AND DATA TYPES: 

--Context channel: 

core_real = { -1..1}

--Decisions: 

datatype decisions = coc | wl | wr | sl | sr 
--We want product type 
--For every pair, we need n-ary pairs, don't we? 

--this helps, this is all combinations, need for all x and ys, to run this: 
--For all combinations, FOR EVERYTHING
--Need n-ary version of this, or, for every combination, 
--this is what we need, 
--

channel context_input:{1..insize}.core_real

channel context_output:Set({coc, wl, wr, sl, sr})


datatype Phases = active


distinct(i, set_pairs) = 
	if card( { x | (x,y) <- set_pairs, x == i}) > 1 
		then 
			false 
		else 
			true 
			
distinct_set(set_pairs) = 
	if member(false, { distinct(i, set_pairs) | i <- {1..card(set_pairs)}})
		then 
			false 
		else 
			true 
--{ x | x <- Set({(n,p) | n <- {1..maxSize}, p <- Phases}), 
							--card(x) == 50 or card(x) == 5 and 
							--distinct_set(x)
							--}
							
channel LayerPhase:{0..layerNo}.union({{(n,x) | n <- {1..5}} | x <- {active}},{{(n,x) | n <- {1..50}} | x <- {active}})

--These are the internal communications now. 
channel NodePhase:{0..layerNo}.{1..maxSize}.Phases

channel end 

weights = 
{
	( (1,1,1), active),
	( (1,1,2), active),
	( (2,1,1), active)
}
	
biases = 
{
	( (1,1), active), 
	( (2,1), active)
}

InputNode(n) = (NodePhase.0.n.active -> SKIP)

InputNodeCollator(node) = let
	C(0, NodeResults) = 
		LayerPhase.0.NodeResults -> SKIP
		
	C(node, NodeResults) = NodePhase.0.node?node_phase -> 
										C((node-1), union({(node, node_phase)}, NodeResults))
	within 
		C(node, {})


InputLayer = 
	(||| i : {1..insize} @ InputNode(i)) 
	[| {| NodePhase.0 |} |] 
	InputNodeCollator(insize)
	
	
EdgeLogic(input_phase, weight, first) = 
	if(first == true) then 
		if(input_phase == active)
			then 
				if(weight == active)
					then 
						active
					else
						active
			else
				if(weight == active)
					then
						active
					else
						active
	else 
		if(input_phase == active)
			then 
				active
			else if (input_phase == active)
				then 
				(if(weight == active)
					then
						active
					else
						active)
			else
				(if(weight == active)
					then
						active
					else
						active)

--Needed to extract a value from singleton set. 
--If its empty, return? Should never be empty, make sure by construction 
--it thinks that the result could be > 1, but it isn't. We need a base case for layer extraction. 
--When creating the channels, FDR thinks that it could be wrong, but it never is, add this pattern to prevent this.
--we use pick for weights, biases, and activations, active, is the base case, this should never happen though.
pick({}) = active
--pick if there is multiple, only possible in theory: 
pick({x}) = x
pick(x) = active

--This should always return a singleton set, because every index should be unique.
--Weights is a function, not a relation. 
--Make sure this is never out of range, otherwise will error.
--WeightExtraction(layer, node, index) = 
	--pick({weight | (weight_index, weight) <- weights, weight_index == (layer,node,index)})

--Replacing with dummy versions for scalability testing, can replcae with one of each if this passes. 
WeightExtraction(layer, node, index) = active 
BiasExtraction(layer, node) = active 

--BiasExtraction(layer, node) = 
	--pick({bias | (bias_index, bias) <- biases, bias_index == (layer,node)})
	
--Applies a weight to a input, this replaces the edge process. 
--If we are on the first layer, we need to extract the weights considering the output of the input nodes could be negative. 
--Every other node, use ReLU law, cannot be negative. 
WeightApplicationSingle((prev_node,result), layer, node) = 
	if(layer == 1)
		then 
			( (prev_node), 
				EdgeLogic(result, WeightExtraction(layer, node, prev_node), true)
			)
		else 
			( (prev_node), 
				EdgeLogic(result, WeightExtraction(layer, node, prev_node), false)
			)

--Need to do this for an entire set of 
--Set comprehension, because this should return the modified set. 
--Set comprehension, we want a modified set, which is weightapplicationsingle. 
--How do we get layer and node? 

WeightApplicationFunction(layer_input, layer, node) = 
	{ WeightApplicationSingle((prev_node,result), layer, node) | 
		(prev_node, result) <- layer_input
	}


--Extracts the node indexed by node, the first argument, from the set of tuples weightedinputs.
--Assumes that weightedinputs is a function, will fail otherwise. 
--If we GET THE EMPTY SET, WE NEED A BASE CASE: 
--This should never happen in reality, when we are running the node, but because 
--we use a parameteric type, the type will allow for different sizes. 
--we will know, if it tries to evaluate node which is greater than weighted inputs. 
--Assumed that the pairs are constructed correctly.

ExtractActivation(node, weightedinputs) = 
	if(node>card(weightedinputs)) 
		then
			active 
		else 
			pick({activation | (node_index, activation) <- weightedinputs, node_index == node})
	
--Have it the same for now, benefits, check really easily if any are, activation is when, bias is negative, if it 
--can't be activated, active. If no weighted nodes are active, 
--Check if is the empty set. for the entire set, 
--If any AREN'T active, active? if any are active, then we might activate, 
--activety ALWAYS REMOVED BY THE WEIGHTING, CAN ONLY BE active, ACTIVE, OR active HERE. Take worst case for activety in weight logic. 
--ActivationLogic(weightedinputs) = 
	--if (card({ (node_index, activation) | (node_index, activation) <- weightedinputs, activation == active}) > 0)
		--then 
			--true
		--else
			--false

ActivationLogic(weightedinputs) = false 
InActivationLogic(weightedinputs) = false

--InActivationLogic(weightedinputs) = 
	--if (card({ (node_index, activation) | (node_index, activation) <- weightedinputs, activation == active}) > 0)
		--then 
			--true
		--else
			--false

--First layer nodes need to be different, 
Node(l, n) = LayerPhase.(l-1)?layer_input -> (
				( BiasExtraction(l, n) == active & ( 
						( ActivationLogic(WeightApplicationFunction(layer_input, l, n)) == false & 
							NodePhase.l.n!active -> SKIP)
						[]
						( ActivationLogic(WeightApplicationFunction(layer_input, l, n)) == true & 
							NodePhase.l.n!active -> SKIP )
							) )
				[]
				( BiasExtraction(l,n) == active & (
					( InActivationLogic(WeightApplicationFunction(layer_input, l, n)) == false & 
						NodePhase.l.n!active -> SKIP)
					[]
					( InActivationLogic(WeightApplicationFunction(layer_input, l, n)) == true & 
						NodePhase.l.n!active -> SKIP )
					) )
				)

--We just need NodeCollator, that is the layer, then to compose the input layers together, then rename them to the context.
--Then done. 


NodeCollator(l) = let
	C(l,0,NodeResults) = 
		LayerPhase.l!NodeResults -> SKIP
		
	C(l,n,NodeResults) = 
		NodePhase.l.n?node_phase -> C(l, (n-1), union({(n, node_phase)}, NodeResults))
		
	within 
		C(l, layerSize(l), {})


--They all need to synchronise, on the layeroutput from previous layer, 

Layer(l) = 
	(([| {| LayerPhase.(l-1) |} |]  i : {1..layerSize(l)} @ Node(l, i) )
	[| {| NodePhase.l |} |] 
	NodeCollator(l))

HiddenLayers = 
	|| i : {1..(layerNo-1)} @
		[ {| LayerPhase.(i-1), LayerPhase.i, NodePhase.i |} ] 
		Layer(i)
		

--OutputLayer = Layer(layerNo)

ANN_Visible = ( (InputLayer [| {| LayerPhase.0 |} |] HiddenLayers)) ; ANN_Visible

assert ANN_Visible :[deadlock-free[FD]]
assert ANN_Visible :[divergence-free]
assert ANN_Visible :[deterministic]

--No hiding for now. 

layerSize(0) = insize
layerSize(layer) = extract_sequence(layer, layerstructure)

extract_sequence(1, sequence) = head(sequence)
extract_sequence(index, sequence) = extract_sequence((index-1), tail(sequence))



	
