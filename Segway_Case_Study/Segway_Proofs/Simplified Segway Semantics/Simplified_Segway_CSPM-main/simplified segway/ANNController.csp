module ANN
exports
	channel angleOutputE:InOut.core_real
	channel adiff:InOut.core_real
	channel anewError:InOut.core_real
	
	channel TEMPORARY_INPUT:{1,2}.core_real

	--These define the basic structure of the model.
	--In_size is the input dimensionality.
	in_size = 2
	--Hl_sizes is a sequence defining the structure of the hidden layers.
	--Every n entry defines a layer with n nodes.
	hl_sizes = <1, 2> 
	--Hl_max is the size of the largest layer.
	hl_max = 2
	--The number of hidden layers, equal to the size of hl_sizes as every entry
	--is a layer.
	hl_size = #hl_sizes
	--The size of the output layer.
	out_size = 1

	--Whether or not to set the initial input, or to iterate over all 
	--possible inputs.
	set_input = True
	--This fixes input, but only allows one value. We need to allow all values and have it input one at 
	--A time though.

	--The weights and biases of the model.
	--If the layer structure is uneven, then redundant sequences
	--for the uneven layers are required for FDR, even though they
	--are not used.
	hl_weights(1,1) = <x | x <- < 0, 0>>
	--In this model, this sequence is redundant, as layer 1 only has
	--one node, but is required as layer 2 has two nodes.
	hl_weights(1,2) = <x | x <- < 0, 0>> 

	hl_weights(2,1) = <x | x <- < 0>> 
	hl_weights(2,2) = <x | x <- < 0>> 

	hl_bias(1) = <x | x <- < 0>> 
	hl_bias(2) = <x | x <- < 0, 0>> 

	out_weights(1) = <x | x <- < 0, 0>>
	out_bias = <x | x <- < 0>>

	--Channels:
	--This section defines the channels used by the entire model.

	Range = {0,1}

	--Hl_Res is used to communicate the result of each node
	--in the hidden layer to the next layer.
	--hl_res is defined by the layer number and the node number.
	--Every node has one output.
	channel hl_res:{0..hl_size}.{1..hl_max}.Range

	--Layer input is used to communicate the layer size to the processes that
	--define the hidden layers. Indexed by the layer size and the maximum size of 
	--each layer. 
	channel layer_input:{0..hl_size}.{1..hl_max} 

	--The layers are indexed by {1..hl_size} in this model, 0 in these channels
	--refer to the input, the size of the input layer_input.0, and the actual 
	--input, hl_res.0. 

	--This channel is only used for the final model, to define the input to the
	-- model clearly.
	channel input:{1..in_size}.Range

	--This channel communicates the individual node inputs (after weight 
	--(application) to the node collator, indexed by layer, node and input.
	channel node_out:{1..hl_size}.{1..hl_max}.{1..hl_max}.Range

	--Weights and bias are used to communicate the hidden layer weights to the
	--node processes, weights are indexed by layer, node and input, where the 
	--input max is also hl_max, as the input size is the previous layer size.
	channel weights:{1..hl_size}.{1..hl_max}.{1..hl_max}.Range
	channel bias:{1..hl_size}.{1..hl_max}.Range

	--Out_weights and out_bias is used to communicate the output layer weights
	--and biases 
	channel out_weights_in:{1..out_size}.{1..hl_max}.Range
	channel out_bias_in:{1..out_size}.Range

	--Internal communication to the collator of the output nodes, indexed by
	--node and input.
	channel out_node_out:{1..out_size}.{1..hl_max}.Range

	--This channel communicates the output of the final hidden layer
	--to the output layer.
	channel out_hl:{1..hl_max}.Range

	--Out is output of the output layer nodes.
	channel out_outputlayer:{1..out_size}.Range

	--Final_out is to print out the result of the output layer clearly, on a
	--seperate dedicated channel.
	channel final_out:{1..out_size}.Range

	--In_node computes input to each node, and applies the binary 
	--weights, input * each weight, it then communicates this along the node_out 
	--channel to collator.
	In_Node(layer, node_in, i) = hl_res.(layer - 1).i?x -> weights.layer.node_in.i?y
								 -> node_out.layer.node_in.i.(x*y) -> SKIP

	--Sign is the sign binary activation function.
	sign(x) = if (x > 0) then 1 else 0

	--Collator is a recursive process that sums all In_node values and provides the 
	--activation function (sign in this case), then communicates this value to the 
	--next layer.
	--Collator stores the cumulative total in the val parameter. 
	Collator(layer, node_in, index, val) = if index == 0 then 
			bias.layer.node_in?b -> hl_res.layer.node_in.sign(val + b) -> SKIP 
			else
			node_out.layer.node_in.index?n -> Collator(layer, node_in, (index-1),
			(val+n))

	--Node is the process for a single node, composed of interleaved In_Nodes 
	--communicating along the node_out channel to Collator. 
	--layer is the index of the current layer, node is the index of this current node, 
	--and input_size, is the size of the previous layer.
	Node(layer, node, input_size) = (|||i:{1..input_size} @ In_Node(layer, node, i))
									[ AIN(layer, node) || AC(layer, node) ] 
									Collator(layer, node, input_size, 0)

	--Weight iterator and bias iterator iterate down the sequences of weights and
	--biases, and communicate them to the Node processes.  
	weight_iterator(_, _, _, <>) = SKIP
	weight_iterator(layer, node, index, <w>^weight_seq) = weights.layer.node.index.w 
						-> weight_iterator(layer, node, (index + 1), weight_seq) 
	 
	bias_iterator(_, _, <>) = SKIP
	--node is 0 here, but the bias sequence not finished?
	bias_iterator(layer, node, <b>^bias_seq) = bias.layer.node.b ->
										bias_iterator(layer, (node + 1), bias_seq)

	--Input process, only used if set_input = True
	--?in_ instead of .in_ means that it will take an input of any value (-1 or 1)
	--Needs channels on hl-res.0
	initial_input_process = TEMPORARY_INPUT.1?in_ -> TEMPORARY_INPUT.2?in2_ -> 
	hl_res.0.1.sign(in_) -> hl_res.0.2.sign(in2_) -> SKIP

	--hl_input_process is all input processes interleaved together, only using initial_input if
	hl_input_process(layer, size, in_size) = if (set_input and layer == 1) then 
											initial_input_process ||| 
											(|||i:{1..size} @ weight_iterator(layer, i, 1, hl_weights(layer, i))) 
											||| 
											bias_iterator(layer, 1, hl_bias(layer))
											 else 
											 (|||i:{1..size} @ weight_iterator(layer, i, 1, hl_weights(layer, i)))
											 ||| bias_iterator(layer, 1, hl_bias(layer))


	--layer_size_process communicates the size of each layer to the layer processes.
	hl_layer_process(_, <>) = SKIP
	hl_layer_process(index, <l> ^ layer_seq) = layer_input.index.l -> hl_layer_process((index + 1), layer_seq)
	layer_size_process = layer_input.0.in_size -> hl_layer_process(1, hl_sizes)

	--Hidden_Layer is the process of one hidden layer, alphabetised parallel Nodes,
	-- which synchronise on the input process.
	Hidden_Layer(layer, size, input_size) = (||i:{1..size} @ [AN(layer, i)] Node(layer, i, input_size))

	--Hidden_Layer_Input is the hidden layer, when it obtains its parameters from the hidden layer input process. 
	Hidden_Layer_Input(layer, size, input_size) = Hidden_Layer(layer, size, input_size) 
												  [AH(layer) || A_input_process(layer)]
												  hl_input_process(layer, size, input_size)

	--Hidden_Layer_LInput is the hidden layer, when it the layer size and input size is obtained from the 
	--layer_input channel.
	Hidden_Layer_LInput(layer) = layer_input.(layer-1)?input_size -> layer_input.layer?size -> 
								 Hidden_Layer_Input(layer, size, input_size)


	--Hidden_Layer_Full is multiple hidden_layer_linput processes in parallel with each other, 
	--communicating along the hl_res channel, this is in paralell with layer_size_process, to communicate
	--the user defined size of each layer.
	Hidden_Layer_Full = (||i:{1..hl_size} @ [AH_LI(i)] Hidden_Layer_LInput(i)) 
						[AH_Full || A_ls_process] 
						layer_size_process

	--link_process takes the output of the last hidden layer and communicates it along the out_hl channel, to
	--communicate this value to the output layer. 
	link_process(index, hl_size) = if index <= hl_size then hl_res.hl_size.index?x -> out_hl.index.x -> 
								   link_process((index+1), hl_size) else SKIP
								   
	--Hidden_Layer_Link communicates the final values along the out_hl channel instead of the hl_res.hl_size 
	--channel, just for clarity when dealing with this input in the output layer.
	Hidden_Layer_Link = (Hidden_Layer_Full 
						[AH_Full || {|hl_res.hl_size, out_hl|}]
						link_process(1, hl_size))
	--Hidden Layer Process Alphabets
	--These are the process alphabets of the hidden layer, used for alphabetised parallel. 

	--Node processes:
	AIN_i(layer, node, i) = {|hl_res.(layer-1).i, weights.layer.node.i, node_out.layer.node.i|}
	AIN(layer, node) = Union({AIN_i(layer, node, x) | x <- {1..hl_max}}) 
	AC(layer, node) = {|node_out.layer.node, bias.layer.node, hl_res.layer.node|}
	AN(layer, node) = union(AIN(layer, node), AC(layer, node))

	--Alphabet of Hidden_Layer.
	AH(layer) = Union({AN(layer, node) | node <- {1..hl_max}})

	--Input processes:
	A_ls_process = {|layer_input|}

	--Node there is a special case for if input is set to manual, then hl_res.0 is communicated, other
	--wise this value is obtained with unguarded input over hl_res.0. 
	A_input_process(layer) = if (set_input and layer == 1) then {|hl_res.0, bias.layer, weights.layer, TEMPORARY_INPUT|} 
							 else {|bias.layer, weights.layer|}
	--Alphabet of the collation processes of the entire hidden layer.
	--Alphabet of hidden layer input.

	AH_I(layer) = union(AH(layer), A_input_process(layer))

	--Alphabet  Hidden_Layer_LInput, layer input.
	AH_LI(layer) = union(AH_I(layer), {|layer_input.layer, layer_input.layer-1|})

	--Alphabet of Hidden_Layer_Full.
	AH_Full = union(Union({AH_LI(layer) | layer <- {1..hl_size}}),
		   A_ls_process)
	--Alphabet of the link process.
	AH_Link = union(AH_Full, {|out_hl|})

	{-Output Layer:
	The output layer functions in exactly the same way as a single
	layer of the hidden layer, seperated for clear definition of the 
	total outputs of the full model, and in many cases the output layer
	has special filters or other features to the output layer, so seperating
	it for multiple types of layers will be an advantage.

	Every alphabet and process is named (input layer name)_O, and is identical
	except for it is configured to only ever be a single layer, and to recieve 
	input from out_hl, and communicates the output on the out channel to the 
	final_out process, defined in the full model section.

	The general structure is:
	Hidden_Layer_Link -> out_hl -> Output_Layer 
	-> out -> (final_out process)
	-}
	--In_Node_O is the output layer's In_Node process, communicating to the Collator_O along
	--the out_node_out channel.
	In_Node_O(node, i) = out_hl.i?x -> out_weights_in.node.i?y -> out_node_out.node.i.(x*y) -> SKIP

	Collator_O(node, index, val) = if index == 0 then out_bias_in.node?b -> out_outputlayer.node.sign(val+b) -> 
								   SKIP else out_node_out.node.index?n -> Collator_O(node, (index-1), (val+n))

	Node_O(node) = (|||i:{1..hl_size} @ In_Node_O(node, i)) [AIN_O(node) || AC_O(node)] Collator_O(node, hl_size, 0)

	weight_iterator_out(_, _, <>) = SKIP
	weight_iterator_out(node, index, <w>^weight_seq) = out_weights_in.node.index.w -> 
													  weight_iterator_out(node, index-1, weight_seq) 

	bias_iterator_out(_, <>) = SKIP
	bias_iterator_out(node, <b>^bias_seq) = out_bias_in.node.b -> bias_iterator_out(node-1, bias_seq)

	out_input_process = (|||i:{1..out_size} @ weight_iterator_out(i, hl_size, out_weights(i)))
						||| 
						bias_iterator_out(out_size, out_bias)

	--Out_Layer is the basic output layer.
	Out_Layer = (||i:{1..out_size} @ [AN_O(i)] Node_O(i))

	--Out_Layer_Input is the output layer in parallel with the input process.
	Out_Layer_Input = Out_Layer [AO || AO_input_process] out_input_process

	--Output_Layer is the full output layer process, taking the output from the
	--hidden layer and producing an output along the out channel.
	Output_Layer = (Hidden_Layer_Link [AH_Link || AO] Out_Layer_Input)


	--Alphabet of output layer processes.
	AIN_O_i(node, i) = {|out_hl.i, out_weights_in.node.i, out_node_out.node.i|}
	AIN_O(node) = Union({AIN_O_i(node, x) | x <- {1..hl_size}})
	AC_O(node) = {|out_bias_in.node, out_outputlayer.node, out_node_out.node|} 
	AN_O(node) = union(AC_O(node), AIN_O(node))

	AO_input_process = {|out_bias_in, out_weights_in|}

	--Alphabet of the complete output layer
	AO = {|out_hl, out_weights_in, out_node_out, out_bias_in, out_outputlayer|}

	final_output(index) = if index == 0 then SKIP else out_outputlayer.index?y -> final_out.index.y -> final_output((index-1))

	--Full is the complete BNN model, hiding all events in all channels except for hl_res.0, the input 
	--channel, and final_out, this process also renames hl_res.0 to input.0. 
	Full = ((Output_Layer [union(AH_Link, AO) || {|out_outputlayer, final_out|}] final_output(out_size)) 
			\ diff(union(AH_Link, AO), {|hl_res.0, final_out, TEMPORARY_INPUT|})) [[hl_res.0.i <- input.i | i <- {1..in_size} ]]
endmodule			
